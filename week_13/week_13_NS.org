#+title: Week 13 NS
#+PROPERTY: header-args:python :session: week-13

* Understanding the Problem.
Looking at the notebook, it mentions things like 5 Why,s decomposition, etc for properly breaking the down the problem for understanding and problem scoping. I think I'll try out the Five Whys for time's sake.
- Why am I doing this? To find solution to quickly & accurately disgonsing breast cancer.
- Why? Because often such symptoms of breast cancer can be hard to detect or might lead to false positives, leading to a misuse of time & resouces. 
- Why? It could be due to a lack of understanding of the possible correlations between certian symptombs and breast cancer.

  Well maybe only three whys, but I now know the importance behind the goal of finding atleast the corrleations between symtombs and breast cancer. 

* Data Setup 
** Data Acquisition & Exploration.
Due to me being given the dataset, I don't have to go looking for it; but if I were to have to, I'd consider research organiztions, open-source sets/kaggle, or maybe just asking the client that this is for to supply some. Besides this, I can move towards exploring the data to make sure the set is clean and properly labled.

#+begin_src python :results output :session week-13 
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt 
from matplotlib.colors import ListedColormap
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier

df = pd.read_csv("/home/nate/NextCloud/Roam/Classes/Intro_to_ML/assignmnets/week_13/[Dataset]_BreastCancer.csv")

print(df)
#+end_src

#+RESULTS:
#+begin_example
           id diagnosis  radius_mean  texture_mean  perimeter_mean  ...  compactness_worst  concavity_worst  concave points_worst  symmetry_worst  fractal_dimension_worst
0      842302         M        17.99           NaN          122.80  ...            0.66560           0.7119                0.2654          0.4601                  0.11890
1      842517         M        20.57         17.77          132.90  ...            0.18660           0.2416                0.1860          0.2750                  0.08902
2    84300903         M        19.69         21.25          130.00  ...            0.42450           0.4504                0.2430          0.3613                  0.08758
3    84348301         M        11.42         20.38           77.58  ...            0.86630           0.6869                0.2575          0.6638                  0.17300
4    84358402         M        20.29         14.34          135.10  ...            0.20500           0.4000                0.1625          0.2364                  0.07678
..        ...       ...          ...           ...             ...  ...                ...              ...                   ...             ...                      ...
564    926424         M        21.56         22.39          142.00  ...            0.21130           0.4107                0.2216          0.2060                  0.07115
565    926682         M        20.13         28.25          131.20  ...            0.19220           0.3215                0.1628          0.2572                  0.06637
566    926954         M        16.60         28.08          108.30  ...            0.30940           0.3403                0.1418          0.2218                  0.07820
567    927241         M        20.60         29.33          140.10  ...            0.86810           0.9387                0.2650          0.4087                  0.12400
568     92751         B         7.76         24.54           47.92  ...            0.06444           0.0000                0.0000          0.2871                  0.07039

[569 rows x 32 columns]
#+end_example

*** After printing some of the features and observations, I don't find anything 'dirty' but I should try doing a more direct search for missing values within the dataset.
#+begin_src python :results output :session week-13
print(df.isnull().values.sum())
print(df.isnull().sum())
#+end_src

#+RESULTS:
#+begin_example
28
id                         0
diagnosis                  0
radius_mean                0
texture_mean               1
perimeter_mean             1
area_mean                  0
smoothness_mean            0
compactness_mean           1
concavity_mean             1
concave points_mean        1
symmetry_mean              0
fractal_dimension_mean     3
radius_se                  0
texture_se                 0
perimeter_se               0
area_se                    0
smoothness_se              3
compactness_se             4
concavity_se               2
concave points_se          1
symmetry_se                3
fractal_dimension_se       0
radius_worst               1
texture_worst              3
perimeter_worst            0
area_worst                 2
smoothness_worst           1
compactness_worst          0
concavity_worst            0
concave points_worst       0
symmetry_worst             0
fractal_dimension_worst    0
dtype: int64
#+end_example

To take note of what I did to figure what I've just found, I had first printed the sum of values which are null within the complete dataframe, showing itself as the the columns which have such observations. Second, I had printed the sum of total null values within those columns, showing how many obvservations are empty.

For time's sake and due to the large amount of other features with filled-in observations, I'm going to drop these null values within their respective rows for clean up.

#+begin_src python :results output :session week-13
df.dropna(inplace=True)
print(df)
#+end_src

#+RESULTS:
#+begin_example
           id diagnosis  radius_mean  texture_mean  perimeter_mean  ...  compactness_worst  concavity_worst  concave points_worst  symmetry_worst  fractal_dimension_worst
1      842517         M        20.57         17.77          132.90  ...            0.18660           0.2416                0.1860          0.2750                  0.08902
2    84300903         M        19.69         21.25          130.00  ...            0.42450           0.4504                0.2430          0.3613                  0.08758
3    84348301         M        11.42         20.38           77.58  ...            0.86630           0.6869                0.2575          0.6638                  0.17300
4    84358402         M        20.29         14.34          135.10  ...            0.20500           0.4000                0.1625          0.2364                  0.07678
5      843786         M        12.45         15.70           82.57  ...            0.52490           0.5355                0.1741          0.3985                  0.12440
..        ...       ...          ...           ...             ...  ...                ...              ...                   ...             ...                      ...
563    926125         M        20.92         25.09          143.00  ...            0.41860           0.6599                0.2542          0.2929                  0.09873
564    926424         M        21.56         22.39          142.00  ...            0.21130           0.4107                0.2216          0.2060                  0.07115
565    926682         M        20.13         28.25          131.20  ...            0.19220           0.3215                0.1628          0.2572                  0.06637
567    927241         M        20.60         29.33          140.10  ...            0.86810           0.9387                0.2650          0.4087                  0.12400
568     92751         B         7.76         24.54           47.92  ...            0.06444           0.0000                0.0000          0.2871                  0.07039

[543 rows x 32 columns]
#+end_example

where I can see that had dropped around 30+ rows!
*** Modifying the Target Column
Looking through dataset, I find that there doesn't exist a pre-defined 'target' column which will be important later for easy of identification during model use; as well, such a colum needs to be in the proper format, like numerical, in order to be used in certain models.

#+begin_src python :results output :session week-13
df = df.rename(columns={'diagnosis':'target'})

sns.countplot(df['target'])
print(df.target.value_counts())
#+end_src

#+RESULTS:
: target
: B    342
: M    201
: Name: count, dtype: int64

So it can be seen that this is done by re-declaring the 'df' object as the image of the method 'rename' which exchanges the column's name 'diagnosis' to 'target'; then I just went and used seaborn to create a simple count plot to show the observations attached to the feature. Notice though how this is the count of B & M, which is to say such observations are not numerical but rather caterogically, and here I was just counting the amount of such categorically observations.

This means I have to convert such a binary classifcation to just that: binary intergers.

#+begin_src python :results output :session week-13
df['target'] = [1 if i.strip()=='M' else 0 for i in df.target]
#+end_src

#+RESULTS:
*** Correlation & Feature Selection.
Finding the correlation between different features within the dataset will help to find such features that are worth looking into and playing with within the models used later; though, with such a dataset with this large of features, it would be better to filter for only the features that reach some high threshold of being correlated with each other, giving a more clear picture of the heavy hitters.

#+begin_src python :results output :session week-13
th = 0.75
corr = df.corr()
filt = np.abs(corr['target']) > th

corr_features = corr.columns[filt].tolist()
sns.clustermap(df[corr_features].corr(), annot=True, fmt='.2f', figsize=(12,8), cmap='YlGnBu')
plt.title("Target's Correlations Greater then " + str(th))
#+end_src

#+RESULTS:

Just to go through this a bit, the main action of filter resides in the object 'corr_features' where I had assigned the filtered columns to the 'corr' object and made such an instance 'corr_features' out of 'corr'.

With this done, I know a bit more what I could see out of the models once trained & tested.
*** Splitting into sets
From here I can start to split the dataframe/set into the needed subsets for training and testing, split by their axes.

#+begin_src python :results output :session week-13 
X = df.drop(['target'], axis=1)
y = df['target']
columns = X.columns.tolist()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)
print('X_train.shape:', X_train.shape)
print('X_test.shape', X_test.shape)
#+end_src

#+RESULTS:
: X_train.shape: (434, 31)
: X_test.shape (109, 31)

not bad, but the difference between those features is insane: I'll standardize the data to decrease this difference.

#+begin_src python :results output :session week-13
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

X_train_df = pd.DataFrame(X_train, columns = columns)
X_train_df["target"] = y_train

print('X_train.shape:', X_train.shape)
print('X_test.shape', X_test.shape)
#+end_src

#+RESULTS:

Before moving forward, it might be a good question to ask two things
- What excatly am I standardizing?
  The values which were standarized was rather the scale of the data rather then the values of the data itself. If one were to look through the data, they would see some of the values being of a very small decimal while other of large intergers: that can be bit of a problem when preforming analysis.
- Why did I set y_train but not y_test to the 'target' feature?
  y_train was assinged here due to it needing to be re-assigned after standardization; y_test is just the 

* Modeling
Finally the fun part, and also the most quickest (how surprising), is running the three models in order to compare the differences between them. Within class, it was hinted that Decisions Trees, one of the models to be ran, should be the most effective in predicting such binary classification. I could see how this might be so due to the similer style which we (us humans) do when diagnosis: it's been working pretty well so far. The other two models I'll be running is SVM and KNNs.

#+begin_src python :results output :session week-13
clf_knn = KNeighborsClassifier(n_neighbors = 2)
clf_knn.fit(X_train, y_train)
#+end_src

#+RESULTS:

#+begin_src python :results output :session week-13
clf_svm = SVC(kernel = 'rbf')
clf_svm.fit(X_train, y_train)
#+end_src

#+begin_src python :results output :session week-13
clf_dt = DecisionTreeClassifier(random_state = 0)
clf_dt.fit(X_train, y_train)
#+end_src

* Model Comparison
With our models trained, I have to compaire such preformance between the two. A method of doing this is to use a confusion matrix and print their respective accuracry score for their training and test sets. Some fancy names, but what do they mean?

- Confusion Matrix
  The confusion matrix is often introduced as the four other derived metrics that come out of it's table: Accuracy, Precision, Recall, F1 Sorce. What the matrix is in isolation is simply the four possiblites of a prediation by the model: True Positive, True Negative, False Positive, False Negative. The Trues are what you think they are to be, but the False are the opposite; meaning they are false predictions of a positive outcome (here maclious) but that isn't actualy true.
- Accuracy
  Just one of the derivable metrics from the Confusion Matrix. This is actually the ratio of the sum of all the correct predictions (TP & TN) over the sum of all predictions, giving the quotient representing the right out of all.

#+begin_src python :results output :session week-13
y_pred_test1 = clf_knn.predict(X_test)
y_pred_train1 = clf_knn.predict(X_train)
cm = confusion_matrix(y_test, y_pred_test1)
acc_test = accuracy_score(y_test, y_pred_test1)
acc_train = accuracy_score(y_train, y_pred_train1)

print("Test Score: {}, Train Score: {}".format(round(acc_test,4), round(acc_train, 4)))
print("CM: \n", cm)
plt.figure(figsize=(3,3))
sns.heatmap(cm, annot=True)
print(classification_report(y_test, y_pred_test1))
#+end_src
  
#+begin_src python :results output :session week-13
y_pred_test2 = clf_svm.predict(X_test)
y_pred_train2 = clf_svm.predict(X_train)
cm2 = confusion_matrix(y_test, y_pred_test2)
acc_test2 = accuracy_score(y_test, y_pred_test2)
acc_train2 = accuracy_score(y_train, y_pred_train2)

print("Test Score: {}, Train Score: {}".format(round(acc_test2,4), round(acc_train2,4)))
print("CM: \n",cm2)
plt.figure(figsize=(3,3))
sns.heatmap(cm2, annot=True)
print(classification_report(y_test, y_pred_test2))
#+end_src

#+begin_src python :results output :session week-13
y_pred_test3 = clf_dt.predict(X_test)
y_pred_train3 = clf_dt.predict(X_train)
cm3 = confusion_matrix(y_test, y_pred_test3)
acc_test3 = accuracy_score(y_test, y_pred_test3)
acc_train3 = accuracy_score(y_train, y_pred_train3)

print("Test Score: {}, Train Score: {}".format(round(acc_test3,4), round(acc_train3,4)))
print("CM: \n",cm3)
plt.figure(figsize=(3,3))
sns.heatmap(cm3, annot=True)
print(classification_report(y_test, y_pred_test3))
#+end_src

So before full reflection, I want to go over what I had just done here. It can be seen that I had implemnted methods to create, record, and visulize such metrics of Accuracy and the matrix which it's derived from. Though they look the same, the differnces can be seen between the object's difference by an interger coinciding with the model used. It's interesting to note that,
- The confusion matrix is created from the y_test set and the predictions from the model choosen trained rather the x_test set; this is the same for the every other model.  
- Accuracry test are done in a similer fashion as the confusion matrix. 


* Reflection
For this reflection portion I'm going to focus review the metrics discussed before for each model. Looking at the confusion matrix...well it's confusing me. Reading a bit deeper into how this confusion matrix is graphed, the rows are to represent the actual cases that belong to either binary classifiction (so bening or malignant) while the columns are the predicted cases for which they belong to either of the binary classifictions. So looking at the the matrix for the SVM for example, the first row representing the actual cases of the classifiction of begin show that the model predicted corectly 77 True Negatives as compaired to ground truth; as well in this row, the model had erroerinsally predicted three cases as being in the class of beign (so a False Positive) when really they're negative. Moving onto the second row which represents the actual cases of the calssification being maligent, the first column shows zero cases which were errornesly predicted to be negative, while the second column in this compairson shows the model had correctly predicted 29 cases as being positive (so being malgent correctly). This is to all say that the SVM model did a pretty good job at correctly predicting cases to be either a True Negative (so truly having begin breast cancer) or True Positive (having unfortnally malicoues breast cancer); but where it really shines is it's ablity to not achive any False Negatives, which would create the situation where someone could be told they are without malicous cancer when they really are! 

Another metric to note is accuracy. Looking towards the favoured SVM model again, it can be seen that it has an accuracy of 0.97 or rather 97%: that's a impressive metric! High enough to indicate strong prediction while not enough to suggest any overfitting is occuring: but there is more to look into as such a metric can be misleading. Looking at the others, I find
- Precision:
  The ratio of True Positives to the sum of all other positives, being true or negative. It's range is from zero to one, where zero means the model fails to have any predicted positives as correct where 1 means every prediction of a postive class was correct. It should be appearnt now that Precision measures accuracy of positive predictions made by classifcation, as it's being done here. Here the Precision for my favoured model is was 96% for benin and 100% or rather 1 for malgianet; this would suggest the model has high accuracy for classifction of binary tumoers and an exceptially high accuracy for classifction of maligent tumors, which seems to coincide with the confusion matrix: the model does not erronsily classify maligent tumors as false negatives while only classify three cases as being maligent when they were binary (so false positive). I'd note here that Precision helps in measuring the preformance of a model's tendency for false positives, which it does do here (though, such a situation here isn't as important as it's inverse).  
- Recall:
  Recall is pretty similer to Preceision where in it's a ratio measuring correct predictions or classifications, but rather Recall divides the total amount of True Positives by the sum of such positives and False negtives, basically asking out of all positives cases, which did the model correctly identity? It has a simlier range as well, acting with the same case where a value of 1 means the model correctly identitifed all positive cases whereas a value of zero means no positive cases were identified. Here the recall for the model was 96% for classifciations of benine tumors and 100% for maligent; this explains why there wasn't any false negatives.  
- F1-Score:
  Before going into what the F1-Score is, it can actually be seen to be taking in both the precision and recall as defined through it's precentages for benign and mailgnant being 98% and 95% respecitly; notice how they are sightly near each other with the class preformance of bengin being slightly higher as I might expect. The F1-Score is the measurment of assessing class preformance of the model, taking in both the desired avoidence of false positrives and false negatrives: but, there is trade off between the two thereby seeking on might dimish the other.

  After going over the three metrics that can be derived from the confusion matrix, there does seem to be measurment proof that the model's accuracy is pushed by the models preformance of classifcation, where such preformance is defined as having a zeroth rate of false negatives and only a small margin of false postives, but a very degree of accruacy for correct classifcation of true postives and true negatives. 
