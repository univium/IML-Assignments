{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09dcdb60-0610-4f70-bbdd-d0cb25a5341c",
   "metadata": {
    "title": "title: Week 6 Ns"
   },
   "source": [
    "\n",
    "\n",
    "* For this assignment I'm going to be using the built in Iris dataset within sci-kit to get a better understanding of how to handle this model type.\n",
    "** To begin, I'll first load the needed functions & other packages needed for this analysis and then split the data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb038e21",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "begin_src python"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "# 1. Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# 2. Split the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61011b63-5fb8-4bbe-8abc-f29f7559aea6",
   "metadata": {
    "title": "RESULTS:"
   },
   "source": [
    "\n",
    "Using this data set, most of the work of creating a dataframe for the dependent & independent variables are done, and likewise the feature & class names are also already abstracted from the complete dataset. Futher on, the sets of data for the independent & dependent variables are futher split into train & test sets for their respective variables types.\n",
    "** It's at this point that model can be trained for the predication of the dependent variable.\n",
    "I'll also include an accuracy metric due to the balanced nature of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3e180c7",
   "metadata": {
    "lines_to_next_cell": 0,
    "title": "begin_src python :results output"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'iris_decision_tree.png'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.metrics import accuracy_score\n",
    "import graphviz\n",
    "\n",
    "# 1. Load the dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "Y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "class_names = iris.target_names\n",
    "\n",
    "# 2. Split the data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 3. Train the model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, Y_train)\n",
    "\n",
    "Y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# 4. Visualize the tree (using the previously corrected method)\n",
    "dot_data = export_graphviz(\n",
    "    dt_model,\n",
    "    feature_names=feature_names,  # The built-in feature names are used directly\n",
    "    class_names=class_names,      # The built-in class names are used directly\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    out_file=None\n",
    ")\n",
    "\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"iris_decision_tree\", format=\"png\", view=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4180627-dd77-4569-9eef-5cecde66823c",
   "metadata": {
    "title": "RESULTS:"
   },
   "source": [
    "\n",
    "\n",
    "After running the model using the training sets for each variable, I've then outputted the results as a graph using graphviz & export_graphciz function within sklearn. Looking over the output, I see that each leaf has five qualites,\n",
    "- Petal Width & Length\n",
    "  This seems to be the defining feature of the different classes of flowers within the set.\n",
    "- Gini\n",
    "  A value which serves to show the all the samples at a given node belonging to some class.\n",
    "- Samples\n",
    "  The amount of samples which then node used in class determination.\n",
    "- Value\n",
    "  A value arrary which shows the training samples from each target class that has reached a node.\n",
    "- Color\n",
    "  Each color corresponds to a target class and here represents the node as such.\n",
    "- Class\n",
    "  The sort of class which the node has predicted to be.\n",
    "\n",
    "The process begins by sorting the Iris setosa class. Any flower with a petal length less than or equal to $2.45$ cm immediately classifies as setosa, marked by an orange node. This split is pure as having a Gini value of zero means all thirty-one samples reaching it are setosa.\n",
    "\n",
    "The remaining seventy-four samples, a mix of versicolor and virginica, are then split using petal width. The majority of versicolor samples branch left, leading to green nodes with low Gini values, indicating high purity for the versicolor class. The virginica samples branch right, resulting in purple nodes.\n",
    "\n",
    "Further, finer splits on both petal length and width are used to clean the mixed nodes. The goal is for every final leaf node to achieve a Gini impurity of zero, confirming the samples are perfectly sorted. At every node, the Value array tracks the count of training samples per class, and the Samples count shows the total data points handled.\n",
    "\n",
    "I'd like to note aswell that the high accuracy is usually a mark of concern, but getting such a accuracy here isn't abnomral due to how simple the dataset being used is.\n",
    "\n",
    "** Review\n",
    "*** I choose this data due to actually failing with another much larger dataset.\n",
    "I wanted something that would be easier to compute so I could better learn what's going on with such a model type; I'd found eailer that using much larger datasets means I'd spend more time trying to make it fit the model rather then actually leraning how it works.\n",
    "*** From what I see, the model seems to have predicted the sort of recursive process needed to figure what sort of flower's class is.\n",
    "The process had as focus,\n",
    "- Petal Length\n",
    "- Petal Width\n",
    "- Sepal Width\n",
    "\n",
    "  These feature also proved to be the most important when making classifications.\n",
    "*** There wasn't much limitations with this type of model for this problem of classification,\n",
    "but I did find that such a model type is hard to use with very large data sets that have a large amount of features; from there the decisions tree can become quite large and hard to understand."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
